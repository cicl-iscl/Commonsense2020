{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Dependencies\n",
    "\n",
    "Ok, so a big part in getting better scores will be to narrow down the problem of the sentence pairs, for this we will need to know important words and their dependents. This notebook will serve solely to work out methods how to get those, so the other ones are not overloaded with stuff (like First Steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "I put a turkey in the fridge\n",
      "I nsubj put VERB []\n",
      "put ROOT put VERB [I, elephant, in]\n",
      "a det elephant NOUN []\n",
      "elephant dobj put VERB [a]\n",
      "in prep put VERB [fridge]\n",
      "the det fridge NOUN []\n",
      "fridge pobj in ADP [the]\n",
      "\n",
      "\n",
      "\n",
      "I nsubj put VERB []\n",
      "put ROOT put VERB [I, turkey, in]\n",
      "a det turkey NOUN []\n",
      "turkey dobj put VERB [a]\n",
      "in prep put VERB [fridge]\n",
      "the det fridge NOUN []\n",
      "fridge pobj in ADP [the]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "test_a = 'I put a elephant in the fridge'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "# for this we want to remove stop words, to be implemented\n",
    "different_words = ['elephant', 'turkey']\n",
    "different_words_idx = [3,4]\n",
    "\n",
    "parse_a = parser(test_a)\n",
    "parse_b = parser(test_b)\n",
    "print(type(parse_a))\n",
    "print(parse_b)\n",
    "for token in parse_a:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n",
    "print('\\n\\n')\n",
    "for token in parse_b:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THis will be methof get_dependents\"\"\"\n",
    "\n",
    "\"\"\"PYTHONIC: maybe it would be cleaner to only do one sentence at a time,\n",
    "and handle the structure of the sentences further up in the logic\"\"\"\n",
    "\n",
    "#print(type(parse_a))\n",
    "def get_dependents(parser, different_words, different_words_idx, sen):\n",
    "    parse_a = parser(sen)\n",
    "    #print(parse_a, different_words)\n",
    "    #print(parse_a)\n",
    "    #for w, idx in zip(different_words, different_words_idx):\n",
    "    for w in different_words.split():\n",
    "        w = parser(w).text\n",
    "        if w == ' ':\n",
    "            continue\n",
    "        idx = get_dependent_index(parse_a, w)\n",
    "        if idx == None:\n",
    "            continue\n",
    "        if w in parse_a.text:\n",
    "            parent = parse_a[idx].head\n",
    "            #print(parent)\n",
    "\n",
    "            # first we check what pos tags our parent has, in order to build a decision tree...\n",
    "            if parent.pos_ == 'VERB':\n",
    "                # now we want all children of the verb with subj and obj labels\n",
    "                verb_dependents_a = []\n",
    "                for child in parent.children:\n",
    "                    if child.dep_[-2:] == 'bj':\n",
    "                        verb_dependents_a.append(child)\n",
    "                    else:\n",
    "                        for nephew in child.children:\n",
    "                            if nephew.dep_[-2:] == 'bj':\n",
    "                                verb_dependents_a.append(nephew)\n",
    "    \n",
    "    try:\n",
    "        return verb_dependents_a\n",
    "    except: return 0\n",
    "    \"\"\"                    \n",
    "    elif w in parse_b.text:\n",
    "        parent = parse_b[idx].head\n",
    "        \n",
    "        # first we check what pos tags our parent has, in order to build a decision tree...\n",
    "        if parent.pos_ == 'VERB':\n",
    "            # now we want all children of the verb with subj and obj labels\n",
    "            verb_dependents_b = []\n",
    "            for child in parent.children:\n",
    "                if child.dep_[-2:] == 'bj':\n",
    "                    verb_dependents_b.append(child)\n",
    "                else:\n",
    "                    for nephew in child.children:\n",
    "                        if nephew.dep_[-2:] == 'bj':\n",
    "                            verb_dependents_b.append(nephew)\n",
    "                            \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(parse_a):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, elephant, fridge]\n",
      "[I, turkey, fridge]\n"
     ]
    }
   ],
   "source": [
    "print(verb_dependents_a)\n",
    "print(verb_dependents_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This will be method compute_similarity\"\"\"\n",
    "#similarityy of two tokens ex:\n",
    "# apples_oranges = apples.similarity(oranges)\n",
    "#dependents = verb_dependents_a\n",
    "def compute_similarity(dependents):\n",
    "    similarity = 0\n",
    "    cur, prev = None, None\n",
    "    for w in dependents:\n",
    "        cur = w\n",
    "        #print(cur)\n",
    "        if prev == None:\n",
    "            prev = cur\n",
    "            continue\n",
    "        #print(prev)\n",
    "        similarity += prev.similarity(cur)\n",
    "        #print(similarity)\n",
    "        prev = cur\n",
    "    return similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49096861109137535"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_a = similarity\n",
    "similarity_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.619660884141922"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_b = similarity\n",
    "similarity_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_dependent_index(parser, sent, word):\n",
    "def get_dependent_index(sent, word):\n",
    "    \n",
    "    #parse = parser(sent)\n",
    "    #print(type(sent))\n",
    "    #print('inside get_dependent_idx:', word)\n",
    "    idx = 0\n",
    "    for i, token in enumerate(sent):\n",
    "        #print( token.text)\n",
    "        if word == token.text:\n",
    "            return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url_data_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv\"\n",
    "data_task_A = pd.read_csv(url_data_task_A,header=0, index_col=0)\n",
    "#data_task_A = np.asarray(data_task_A)\n",
    "different_words = pd.read_csv('different_words.csv', index_col=0)\n",
    "#different_words = np.asarray(different_words)\n",
    "different_words_idx = np.load('idx_different_words.npy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orange juice</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a mile</td>\n",
       "      <td>100 , 000 miles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##s me</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                 1\n",
       "0   orange juice              milk\n",
       "1          apple              milk\n",
       "2         a mile   100 , 000 miles\n",
       "3         ##s me                 i"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_words[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He poured orange juice on his cereal. He poured milk on his cereal.\n",
      "[He, juice, cereal] [He, milk, cereal]\n",
      "0.6528310328722 0.6681381613016129\n",
      "He drinks apple. He drinks milk.\n",
      "[He, apple] [He, milk]\n",
      "0.1234443187713623 0.1834234744310379\n",
      "Jeff ran a mile today Jeff ran 100,000 miles today\n",
      "[Jeff, mile] [Jeff]\n",
      "0.18877871334552765 0\n",
      "A mosquito stings me I sting a mosquito\n",
      "0 0\n",
      "0.18877871334552765 0\n",
      "A niece is a person. A giraffe is a person.\n",
      "0 0\n",
      "0.18877871334552765 0\n",
      "A walk-in closet is larger than a normal closet. A normal closet is larger than a walk-in closet.\n",
      "0 []\n",
      "0.18877871334552765 0\n",
      "I like to ride my chocolate I like to ride my bike\n",
      "[chocolate] [bike]\n",
      "0 0\n",
      "A GIRL WON THE RACE WITH HER FRIEND A GIRL WON THE RACE WITH HORSE\n",
      "0 0\n",
      "0 0\n",
      "he put elephant into the jug he pour water in to the jug\n",
      "[he, elephant, jug] [he, water, jug]\n",
      "0.44731343537569046 0.27087930403649807\n",
      "A girl plays volleyball A dog plays volleyball\n",
      "[girl, volleyball] [dog, volleyball]\n",
      "0.25031012296676636 0.39596953988075256\n",
      "Eggs eat kis on Easter. Kids find eggs on Easter.\n",
      "[Eggs, kis, Easter] [Kids, eggs, Easter]\n",
      "0.08138547837734222 0.6069076955318451\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "for tuple_1, tuple_2 in zip(data_task_A.iterrows(),different_words.iterrows()):\n",
    "    sen_0 = tuple_1[1]['sent0']\n",
    "    sen_1 = tuple_1[1]['sent1']\n",
    "    \n",
    "    dif_0 = tuple_2[1][0]\n",
    "    dif_1 = tuple_2[1][1]\n",
    "    \n",
    "    #print(dif_w_idx_0, dif_w_idx_1)\n",
    "    dif_w_idx_0 = different_words_idx[tuple_1[0]][0]\n",
    "    dif_w_idx_1 = different_words_idx[tuple_1[0]][1]\n",
    "    #print(sen_0,dif_0,dif_w_idx_0,dif_w_idx_1)\n",
    "    \n",
    "    dependents_0 = get_dependents(parser, dif_0, dif_w_idx_0, sen_0)\n",
    "    dependents_1 = get_dependents(parser, dif_1, dif_w_idx_1, sen_1)\n",
    "    \n",
    "    if not type(dependents_0) == int:\n",
    "        #print(type(dependents_0))\n",
    "        if len(dependents_0) > 0:\n",
    "            sim_0 = compute_similarity(dependents_0)\n",
    "            sim_1 = compute_similarity(dependents_1)\n",
    "    \n",
    "    print(sen_0,sen_1)\n",
    "    print(dependents_0, dependents_1)\n",
    "    print(sim_0, sim_1)\n",
    "    if tuple_1[0] == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependents_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
