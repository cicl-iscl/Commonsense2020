{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Dependencies\n",
    "\n",
    "Ok, so a big part in getting better scores will be to narrow down the problem of the sentence pairs, for this we will need to know important words and their dependents. This notebook will serve solely to work out methods how to get those, so the other ones are not overloaded with stuff (like First Steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I put a elephant in the fridge\n",
      "I put a turkey in the fridge\n",
      "I nsubj put VERB []\n",
      "put ROOT put VERB [I, elephant, in]\n",
      "a det elephant NOUN []\n",
      "elephant dobj put VERB [a]\n",
      "in prep put VERB [fridge]\n",
      "the det fridge NOUN []\n",
      "fridge pobj in ADP [the]\n",
      "\n",
      "\n",
      "\n",
      "I nsubj put VERB []\n",
      "put ROOT put VERB [I, turkey, in]\n",
      "a det turkey NOUN []\n",
      "turkey dobj put VERB [a]\n",
      "in prep put VERB [fridge]\n",
      "the det fridge NOUN []\n",
      "fridge pobj in ADP [the]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "test_a = 'I put a elephant in the fridge'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "# for this we want to remove stop words\n",
    "# !!! We also want to know the index of the different words, HAS TO BE IMPLEMENTED!!!\n",
    "different_words = ['elephant', 'turkey']\n",
    "different_words_idx = [3,4]\n",
    "\n",
    "parse_a = parser(test_a)\n",
    "parse_b = parser(test_b)\n",
    "print(parse_a)\n",
    "print(parse_b)\n",
    "for token in parse_a:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n",
    "print('\\n\\n')\n",
    "for token in parse_b:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put\n"
     ]
    }
   ],
   "source": [
    "\"\"\" THis will be methof get_dependents\"\"\"\n",
    "\n",
    "\"\"\"PYTHONIC: maybe it would be cleaner to only do one sentence at a time,\n",
    "and handle the structure of the sentences further up in the logic\"\"\"\n",
    "\n",
    "#print(type(parse_a))\n",
    "for w, idx in zip(different_words, different_words_idx):\n",
    "    w = parser(w).text\n",
    "    \n",
    "    if w in parse_a.text:\n",
    "        parent = parse_a[idx].head\n",
    "        print(parent)\n",
    "        \n",
    "        # first we check what pos tags our parent has, in order to build a decision tree...\n",
    "        if parent.pos_ == 'VERB':\n",
    "            # now we want all children of the verb with subj and obj labels\n",
    "            verb_dependents_a = []\n",
    "            for child in parent.children:\n",
    "                if child.dep_[-2:] == 'bj':\n",
    "                    verb_dependents_a.append(child)\n",
    "                else:\n",
    "                    for nephew in child.children:\n",
    "                        if nephew.dep_[-2:] == 'bj':\n",
    "                            verb_dependents_a.append(nephew)\n",
    "                            \n",
    "    elif w in parse_b.text:\n",
    "        parent = parse_b[idx].head\n",
    "        \n",
    "        # first we check what pos tags our parent has, in order to build a decision tree...\n",
    "        if parent.pos_ == 'VERB':\n",
    "            # now we want all children of the verb with subj and obj labels\n",
    "            verb_dependents_b = []\n",
    "            for child in parent.children:\n",
    "                if child.dep_[-2:] == 'bj':\n",
    "                    verb_dependents_b.append(child)\n",
    "                else:\n",
    "                    for nephew in child.children:\n",
    "                        if nephew.dep_[-2:] == 'bj':\n",
    "                            verb_dependents_b.append(nephew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, elephant, fridge]\n",
      "[I, turkey, fridge]\n"
     ]
    }
   ],
   "source": [
    "print(verb_dependents_a)\n",
    "print(verb_dependents_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "turkey\n",
      "I\n",
      "0.10982343554496765\n",
      "fridge\n",
      "turkey\n",
      "0.619660884141922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This will be method compute_similarity\"\"\"\n",
    "#similarityy of two tokens ex:\n",
    "# apples_oranges = apples.similarity(oranges)\n",
    "dependents = verb_dependents_b\n",
    "similarity = 0\n",
    "cur, prev = None, None\n",
    "for w in dependents:\n",
    "    cur = w\n",
    "    print(cur)\n",
    "    if prev == None:\n",
    "        prev = cur\n",
    "        continue\n",
    "    print(prev)\n",
    "    similarity += prev.similarity(cur)\n",
    "    print(similarity)\n",
    "    prev = cur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49096861109137535"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_a = similarity\n",
    "similarity_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.619660884141922"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_b = similarity\n",
    "similarity_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
