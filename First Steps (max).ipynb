{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in Computational Linguistics\n",
    "## SemEval 2020: Commonsense Validation and Explanation\n",
    "\n",
    "We're participating in task 4 of the SemEval 2020 Challenges for our seminar Challenges in Computational Linguistics, University TÃ¼bingen.\n",
    "\n",
    "This notebook is meant as playground and first steps, to get the ball rolling. It can later be used as a template to build the final notebook (or program)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "### Read-in for task 1\n",
    "\n",
    "First I use the given data from the tasks github respo for subtask A.\n",
    "\n",
    "https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/tree/master/Training%20%20Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports for data\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He poured orange juice on his cereal.</td>\n",
       "      <td>He poured milk on his cereal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He drinks apple.</td>\n",
       "      <td>He drinks milk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeff ran a mile today</td>\n",
       "      <td>Jeff ran 100,000 miles today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    sent0                          sent1\n",
       "id                                                                      \n",
       "0   He poured orange juice on his cereal.  He poured milk on his cereal.\n",
       "1                        He drinks apple.                He drinks milk.\n",
       "2                   Jeff ran a mile today   Jeff ran 100,000 miles today"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Read in the data directly from github\"\"\"\n",
    "url_data_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv\"\n",
    "url_answers_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv\"\n",
    "\n",
    "data_task_A = pd.read_csv(url_data_task_A,header=0, index_col=0)\n",
    "answers_task_A = pd.read_csv(url_answers_task_A, index_col=0)\n",
    "\n",
    "data_task_A[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> shape of data: (10000, 2) shape of answers: (9999, 1) one line is missing \n",
      "because no header here\n",
      "\n",
      "To get first column, first row: He poured orange juice on his cereal.\n",
      "\n",
      "To get both colums for given row: sent0    He poured orange juice on his cereal.\n",
      "sent1            He poured milk on his cereal.\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check data type, shape, etc\n",
    "print(type(data_task_A), 'shape of data:',data_task_A.shape, 'shape of answers:',\n",
    "      answers_task_A.shape, 'one line is missing \\nbecause no header here\\n')\n",
    "\n",
    "print('To get first column, first row:', data_task_A['sent0'].iloc[0]) # iloc only takes integers\n",
    "print('\\nTo get both colums for given row:',data_task_A.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be enough in respect to the data for now, next step is manipuating the data to our needs.\n",
    "\n",
    "## Using spacy for Natural Language Processing\n",
    "### What about NLTK ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now the fun starts...\"\"\"\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/max/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking and loading stuff...\n",
    "nltk.__version__\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenized DataFrame\n",
    "\n",
    "With help of Pythons list comprehension, we're transforming the string sentences into list of tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sentence=pd.DataFrame([[nltk.word_tokenize(row['sent0']), nltk.word_tokenize(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[He, poured, orange, juice, on, his, cereal, .]</td>\n",
       "      <td>[He, poured, milk, on, his, cereal, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[He, drinks, apple, .]</td>\n",
       "      <td>[He, drinks, milk, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Jeff, ran, a, mile, today]</td>\n",
       "      <td>[Jeff, ran, 100,000, miles, today]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sent0  \\\n",
       "0  [He, poured, orange, juice, on, his, cereal, .]   \n",
       "1                           [He, drinks, apple, .]   \n",
       "2                      [Jeff, ran, a, mile, today]   \n",
       "\n",
       "                                    sent1  \n",
       "0  [He, poured, milk, on, his, cereal, .]  \n",
       "1                   [He, drinks, milk, .]  \n",
       "2      [Jeff, ran, 100,000, miles, today]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of distinct words\n",
    "\n",
    "This could be done together with the above tokenization inside one loop, for better readability, I separated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_dist_words = nltk.FreqDist()\n",
    "\n",
    "# there should be a way to skip the double lookup...\n",
    "for i, (row['sent0'], row['sent1']) in tokens_per_sentence.iterrows():\n",
    "    for word1, word2 in zip(row['sent0'], row['sent1']):\n",
    "        \n",
    "        # if it's the same word, we don't want to count it as double\n",
    "        if word1.lower() == word2.lower():\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "        else:\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "            number_dist_words[word2.lower()] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct words: 8078\n",
      "Output first 10 words:\n",
      "\n",
      "\"he\" occures 1782 number of times.\n",
      "\"poured\" occures 20 number of times.\n",
      "\"orange\" occures 17 number of times.\n",
      "\"milk\" occures 110 number of times.\n",
      "\"juice\" occures 26 number of times.\n",
      "\"on\" occures 1098 number of times.\n",
      "\"his\" occures 859 number of times.\n",
      "\"cereal\" occures 11 number of times.\n",
      "\".\" occures 3560 number of times.\n",
      "\"drinks\" occures 40 number of times.\n"
     ]
    }
   ],
   "source": [
    "print('number of distinct words:', len(number_dist_words))\n",
    "print('Output first 10 words:\\n')\n",
    "i = 0\n",
    "for key, val in number_dist_words.items():\n",
    "    print('\"{}\"'.format(key), 'occures', val, 'number of times.')\n",
    "    i += 1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagger\n",
    "\n",
    "A quick implementation of a part-of-speech-tagger. This again could be done inside the above forloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/max/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_per_sentence = pd.DataFrame([[nltk.pos_tag(row['sent0']), nltk.pos_tag(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(I, PRP), (have, VBP), (a, DT), (desk, NN), (...</td>\n",
       "      <td>[(I, PRP), (have, VBP), (a, DT), (lamp, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent0  \\\n",
       "0  [(I, PRP), (have, VBP), (a, DT), (desk, NN), (...   \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...   \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...   \n",
       "\n",
       "                                               sent1  \n",
       "0  [(I, PRP), (have, VBP), (a, DT), (lamp, NN), (...  \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...  \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
