{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in Computational Linguistics\n",
    "## SemEval 2020: Commonsense Validation and Explanation\n",
    "\n",
    "We're participating in task 4 of the SemEval 2020 Challenges for our seminar Challenges in Computational Linguistics, University TÃ¼bingen.\n",
    "\n",
    "This notebook is meant as playground and first steps, to get the ball rolling. It can later be used as a template to build the final notebook (or program)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "### Read-in for task 1\n",
    "\n",
    "First I use the given data from the tasks github respo for subtask A.\n",
    "\n",
    "https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/tree/master/Training%20%20Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports for data\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    sent0                          sent1\n",
      "id                                                                      \n",
      "0   He poured orange juice on his cereal.  He poured milk on his cereal.\n",
      "1                        He drinks apple.                He drinks milk.\n",
      "2                   Jeff ran a mile today   Jeff ran 100,000 miles today\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0   \n",
       "0  0\n",
       "1  0\n",
       "2  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Read in the data directly from github\"\"\"\n",
    "url_data_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv\"\n",
    "url_answers_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv\"\n",
    "\n",
    "data_task_A = pd.read_csv(url_data_task_A,header=0, index_col=0)\n",
    "answers_task_A = pd.read_csv(url_answers_task_A, index_col=0, header=None)\n",
    "\n",
    "print(data_task_A[:3])\n",
    "answers_task_A[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> shape of data: (10000, 2) shape of answers: (9999, 1) one line is missing \n",
      "because no header here\n",
      "\n",
      "To get first column, first row: He poured orange juice on his cereal.\n",
      "\n",
      "To get both colums for given row: sent0    He poured orange juice on his cereal.\n",
      "sent1            He poured milk on his cereal.\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check data type, shape, etc\n",
    "print(type(data_task_A), 'shape of data:',data_task_A.shape, 'shape of answers:',\n",
    "      answers_task_A.shape, 'one line is missing \\nbecause no header here\\n')\n",
    "\n",
    "print('To get first column, first row:', data_task_A['sent0'].iloc[0]) # iloc only takes integers\n",
    "print('\\nTo get both colums for given row:',data_task_A.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Using spacy for Natural Language Processing\n",
    "### What about NLTK ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now the fun starts...\"\"\"\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/max/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking and loading stuff...\n",
    "nltk.__version__\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenized DataFrame\n",
    "\n",
    "With help of Pythons list comprehension, we're transforming the string sentences into list of tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sentence=pd.DataFrame([[nltk.word_tokenize(row['sent0']), nltk.word_tokenize(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[He, poured, orange, juice, on, his, cereal, .]</td>\n",
       "      <td>[He, poured, milk, on, his, cereal, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[He, drinks, apple, .]</td>\n",
       "      <td>[He, drinks, milk, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Jeff, ran, a, mile, today]</td>\n",
       "      <td>[Jeff, ran, 100,000, miles, today]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sent0  \\\n",
       "0  [He, poured, orange, juice, on, his, cereal, .]   \n",
       "1                           [He, drinks, apple, .]   \n",
       "2                      [Jeff, ran, a, mile, today]   \n",
       "\n",
       "                                    sent1  \n",
       "0  [He, poured, milk, on, his, cereal, .]  \n",
       "1                   [He, drinks, milk, .]  \n",
       "2      [Jeff, ran, 100,000, miles, today]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of distinct words\n",
    "\n",
    "This could be done together with the above tokenization inside one loop, for better readability, I separated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_dist_words = nltk.FreqDist()\n",
    "\n",
    "# there should be a way to skip the double lookup...\n",
    "for i, rows in tokens_per_sentence.iterrows():\n",
    "    for word1, word2 in zip(rows['sent0'], rows['sent1']):\n",
    "        \n",
    "        # if it's the same word, we don't want to count it as double\n",
    "        if word1.lower() == word2.lower():\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "        else:\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "            number_dist_words[word2.lower()] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct words: 8078\n",
      "Output first 10 words:\n",
      "\n",
      "\"he\" occures 1782 number of times.\n",
      "\"poured\" occures 20 number of times.\n",
      "\"orange\" occures 17 number of times.\n",
      "\"milk\" occures 110 number of times.\n",
      "\"juice\" occures 26 number of times.\n",
      "\"on\" occures 1098 number of times.\n",
      "\"his\" occures 859 number of times.\n",
      "\"cereal\" occures 11 number of times.\n",
      "\".\" occures 3560 number of times.\n",
      "\"drinks\" occures 40 number of times.\n"
     ]
    }
   ],
   "source": [
    "print('number of distinct words:', len(number_dist_words))\n",
    "print('Output first 10 words:\\n')\n",
    "i = 0\n",
    "for key, val in number_dist_words.items():\n",
    "    print('\"{}\"'.format(key), 'occures', val, 'number of times.')\n",
    "    i += 1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagger\n",
    "\n",
    "A quick implementation of a part-of-speech-tagger. This again could be done inside the above forloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/max/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_per_sentence = pd.DataFrame([[nltk.pos_tag(row['sent0']), nltk.pos_tag(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...</td>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent0  \\\n",
       "0  [(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...   \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...   \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...   \n",
       "\n",
       "                                               sent1  \n",
       "0  [(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...  \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...  \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "### BllipParser\n",
    "\n",
    "\n",
    "Let's use, for simplicity purposes, the bllip parser module that comes with the nltk package.\n",
    "\n",
    "Sadly, this didn't work in the beginning, so I quickly downloaded the bllipparser directly from their github:\n",
    "https://github.com/BLLIP/bllip-parser and\n",
    "https://github.com/BLLIP/bllip-parser/blob/master/README-python.rst\n",
    "\n",
    "If you want to try the next few lines, make sure to follow the steps and install the package and WSJ+Gigaword-v2 Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.bllip import BllipParser # this doesn't work: ImportError: Couldn't import bllipparser module: No module named 'bllipparser'\n",
    "from bllipparser import RerankingParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_parser = BllipParser('/Users/max/.local/share/bllipparser/WSJ+Gigaword-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_path = '/Users/max/.local/share/bllipparser/WSJ+Gigaword-v2'\n",
    "\n",
    "\"\"\"This is a time consuming operation (1min)\"\"\"\n",
    "parser = RerankingParser.from_unified_model_dir(parser_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'En', 'case_insensitive': False, 'nbest': 2, 'small_corpus': True, 'overparsing': 21, 'debug': 0, 'smooth_pos': 0}\n",
      "I put an elephant in the fridge\n",
      "2 x\n",
      "-15.839081581410 -63.140991134783\n",
      "(S1 (S (NP (PRP I)) (VP (VBP put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "-16.692901096060 -65.869600587365\n",
      "(S1 (S (NP (PRP I)) (VP (VB put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "\n",
      "I put a turkey in the fridge\n",
      "2 x\n",
      "-15.839081581410 -63.140991134783\n",
      "(S1 (S (NP (PRP I)) (VP (VBP put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "-16.692901096060 -65.869600587365\n",
      "(S1 (S (NP (PRP I)) (VP (VB put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "parser.parse outputs a list of N most probable parses, default n = 50\n",
    "parser.set_parser_options(nbest=10) sets new options and outputs a dict with current ones\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parser_options = parser.set_parser_options(nbest=2)\n",
    "print(parser_options)\n",
    "\n",
    "test_a = 'I put an elephant in the fridge'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "parsed_sen_a = parser.parse(test_a)\n",
    "parsed_sen_b = parser.parse(test_b)\n",
    "print(test_a)\n",
    "print(parsed_sen_b)\n",
    "print(test_b)\n",
    "print(parsed_sen_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary BllipParser\n",
    "As we can see, we get the same scores for both sentences. This parser is useful for getting the dependencies of the tokens, or rather the grammatical structure.\n",
    "\n",
    "While this can certainly help us, we additionally want a parser that computs the propability P of a word W2, given a W1: P(W2|W1). Then We can compute P(S) for sentence S = W1 W2 W3 as P(S)=P(W1|start)+P(W2|W1)+P(W3|W2)+P(end|W3)\n",
    "\n",
    "BTW; I'm not sure on that probability calculation above, i think it differs for depenendent/independent events, so we have to check that again.\n",
    "\n",
    "We can probably make more use of the sentence structure for task B\n",
    "\n",
    "### StanfordParser\n",
    "\n",
    "As suggested by Cagri, we could also make use of the StanfordParser. It usually runs on Java, so we have to make some adjustments to our python script if we want to use it. I found couple good-looking lines from: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "Also on the official site https://nlp.stanford.edu/software/lex-parser.shtml#Download an interface to python is linked:http://projects.csail.mit.edu/spatial/Stanford_Parser which I have yet to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT an) (NN elephant))\n",
      "      (PP (IN in) (NP (DT the) (NN fridge))))))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "java_path = r'/Library/Java/JavaVirtualMachines/jdk-10.jdk/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "stanford_parser = StanfordParser(path_to_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "# we again use test sentences test_a & test_b from above\n",
    "result = list(stanford_parser.raw_parse(test_a))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try out the CoreNLPParser as suggested in the DeprecationWarning above.\n",
    "\n",
    "I tried drawing the tree like the tutorial linked above suggsted, but ran into some dependency-problems which I wont delve into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT an) (NN elephant))\n",
      "      (PP (IN in) (NP (DT the) (NN fridge))))))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"StanfordDependencyParser\"\"\"\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "depend_parser = StanfordParser(path_to_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "result = list(depend_parser.raw_parse(test_a))  \n",
    "print(result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary StanfordParser\n",
    "\n",
    "Seems like both run on the CoreNLPParser, so we should look into this one. Also we should research if these Parser can also output the propability we actually want out of our parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "The thing always going through my mind with a propability calculation of the whole sentence is called a Language Model (should've known that) and spacy provides us with a pretrained one...\n",
    "\n",
    "Alright, following I'm trying to implement the BERT model to compute word embeddings for three words out of our dataset. Pipeline will be as follows:\n",
    "\n",
    "- go through dataset and see which words differ\n",
    "- get dependent words through pos tags\n",
    "- run these words through BERT\n",
    "- get word embeddings as output\n",
    "- compute vector distance\n",
    "- whichever distance is lower is better\n",
    "- output is a measure of the difference of the distances\n",
    "\n",
    "All of this (without first, maybe second step) cann happen in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://github.com/huggingface/transformers\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "# FullTokenizer from file tokenization.py, method copied from bert-github:https://github.com/google-research/bert\n",
    "# with tensorflow 2, the normal implementation doesnt work anymore, you have to run the upgrade script on the file.\n",
    "import tokenization_v2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different words\n",
    "We want some more information about our data like\n",
    "- longest word sequence\n",
    "- word pairs that differ between the sentences\n",
    "- dependents on the different words (looking at POS-tags should we a way to find them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, None], [None, None], [None, None], [None, None], [None, None]]\n",
      "(5, 2) None\n",
      "[[0], None]\n",
      "[[1], [1]]\n",
      "[[2], [2]]\n",
      "[[3], [3]]\n",
      "[[4], [4]]\n",
      "[[list([0]) list([1])]\n",
      " [list([1]) list([2])]\n",
      " [list([2]) list([3])]\n",
      " [list([3]) list([4])]\n",
      " [list([4]) list([5])]] \n",
      " [[[4], [5]], [[4], [5]], [[4], [5]], [[4], [5]], [[4], [5]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\na = [0,1,2]\\nb = [[3,4],[5,6]]\\nc = [[[7,8],[9,10]],[[11,12],[13,14]]]\\n\\nprint(a[1])\\nprint(b[0][1])\\nprint(c[0][1][1])\\nprint(np.asarray(c))\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [[None, None]] * 5\n",
    "k = np.asarray(l)\n",
    "#print(l[1:2],'\\n\\n\\n')\n",
    "print(k.shape, print(l))\n",
    "for i, ele in enumerate(k):\n",
    "    k[i][0] = [i]\n",
    "    k[i][1] = [i+1]\n",
    "\n",
    "for i, ele in enumerate(l):\n",
    "    l[i][0] = [i]\n",
    "    l[i][1] = [i+1]\n",
    "    \n",
    "    \n",
    "#print('here',l)\n",
    "#l[0][0] = [0]\n",
    "print(k, '\\n', l)\n",
    "\n",
    "\"\"\"\n",
    "a = [0,1,2]\n",
    "b = [[3,4],[5,6]]\n",
    "c = [[[7,8],[9,10]],[[11,12],[13,14]]]\n",
    "\n",
    "print(a[1])\n",
    "print(b[0][1])\n",
    "print(c[0][1][1])\n",
    "print(np.asarray(c))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['[CLS]', 'he', 'poured', 'orange', 'juice', 'on', 'his', 'cereal', '.', '[SEP]'], ['[CLS]', 'he', 'poured', 'milk', 'on', 'his', 'cereal', '.', '[SEP]']], [['[CLS]', 'he', 'drinks', 'apple', '.', '[SEP]'], ['[CLS]', 'he', 'drinks', 'milk', '.', '[SEP]']], [['[CLS]', 'jeff', 'ran', 'a', 'mile', 'today', '[SEP]'], ['[CLS]', 'jeff', 'ran', '100', ',', '000', 'miles', 'today', '[SEP]']]] \n",
      "Longest sentence length: 25 \n",
      "First three word-pairs that differ: [[list(['orange', 'juice']) list(['milk'])]\n",
      " [list(['apple']) list(['milk'])]\n",
      " [list(['a', 'mile']) list(['100', ',', '000', 'miles'])]] \n",
      "shape of different_words: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "test_a = 'I put an elephant in the fridge'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "\"\"\"We use the official FullTokenizer just to make sure\"\"\"\n",
    "tokenizer = tokenization_v2.FullTokenizer('BERT/vocab.txt')\n",
    "sen_tok = tokenizer.tokenize(test_a)\n",
    "\n",
    "toks_task_A = []\n",
    "max_seq_len = 0\n",
    "different_words = np.asarray([[None,None]] * (data_task_A.shape[0]))\n",
    "\n",
    "# data_task_A, answers_task_A\n",
    "for index, row in data_task_A.iterrows():\n",
    "    \n",
    "    sen_0_tok = tokenizer.tokenize(row['sent0'])\n",
    "    sen_1_tok = tokenizer.tokenize(row['sent1'])\n",
    "    \n",
    "    len_sen_0_tok = len(sen_0_tok)\n",
    "    len_sen_1_tok = len(sen_1_tok)\n",
    "    \n",
    "    \"\"\"Here we get the longest token-sequence length\"\"\"\n",
    "    if len_sen_0_tok >= len_sen_1_tok:\n",
    "        if len_sen_0_tok > max_seq_len:\n",
    "            max_seq_len = len_sen_0_tok\n",
    "    elif len_sen_1_tok > max_seq_len:\n",
    "            max_seq_len = len_sen_1_tok\n",
    "    \n",
    "    \"\"\"Here we look for the words that differ\"\"\"\n",
    "    if len_sen_0_tok == len_sen_1_tok:\n",
    "        for tok_0, tok_1 in zip(sen_0_tok, sen_1_tok):\n",
    "            if tok_0 != tok_1:\n",
    "\n",
    "                different_words[index][0] = [tok_0]\n",
    "                different_words[index][1] = [tok_1]\n",
    "                \n",
    "    # We should make sure that the different words stay in the spots for sent0 sent1,\n",
    "    else:\n",
    "        \n",
    "        if len_sen_0_tok > len_sen_1_tok:\n",
    "            different_words[index][0] = [x for x in sen_0_tok if x not in sen_1_tok]\n",
    "            different_words[index][1] = [x for x in sen_1_tok if x not in sen_0_tok]\n",
    "        else:\n",
    "            different_words[index][0] = [x for x in sen_0_tok if x not in sen_1_tok]\n",
    "            different_words[index][1] = [x for x in sen_1_tok if x not in sen_0_tok]\n",
    "            \n",
    "        \n",
    "    sen_0_tok.insert(0,'[CLS]')\n",
    "    sen_0_tok.append('[SEP]')\n",
    "    sen_1_tok.insert(0, '[CLS]')\n",
    "    sen_1_tok.append('[SEP]')\n",
    "    \n",
    "    toks_task_A.append([sen_0_tok,sen_1_tok])\n",
    "\n",
    "print(toks_task_A[:3], '\\nLongest sentence length:', max_seq_len, \n",
    "      '\\nFirst three word-pairs that differ:', different_words[:3],\n",
    "      '\\nshape of different_words:', np.asarray(different_words).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['orange', 'juice']) list(['milk'])]\n",
      "[list(['apple']) list(['milk'])]\n",
      "[list(['a', 'mile']) list(['100', ',', '000', 'miles'])]\n",
      "[list(['##s', 'me']) list(['i'])]\n",
      "[list(['niece']) list(['gi', '##raf', '##fe'])]\n"
     ]
    }
   ],
   "source": [
    "for i, ele in enumerate(different_words):\n",
    "    print(ele)\n",
    "    if i == 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saver below loads the graph-operations of pretrained model.\n",
    "First we need to load the meta graph, then the weights.\n",
    "\"\"\"\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.import_meta_graph('BERT/bert_model.ckpt.meta')\n",
    "saver.restore(sess, 'BERT/bert_model.ckpt')\n",
    "\n",
    "\"\"\"\n",
    "the saver imports the graph of the saved model and with restore, loads the weights into the different operations.\n",
    "So if we want to load the model from file and do a classification with it, we need to modify the graph.\n",
    "Precisely, we need to add 3 Input layers (operations) like two cells below from: https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
    "Also a classification ops like https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with tensorflow hub\n",
    "\n",
    "Because working directly with the graph and operations can be quite tiresome, we'll try tensorflow hub from https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22. This promises us quicker results. Ill still want to figure out how to work with the graph later in the game.\n",
    "\n",
    "Also this tutorial works with tensorflow 2, so make sure you have that installed.\n",
    "Check with tensorflow.\\__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_seq_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aa53ac6435c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_len\u001b[0m  \u001b[0;31m# We take the max length from above cell \"different words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n\u001b[1;32m      4\u001b[0m                                        name=\"input_word_ids\")\n\u001b[1;32m      5\u001b[0m input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_seq_len' is not defined"
     ]
    }
   ],
   "source": [
    "max_seq_length = max_seq_len  # We take the max length from above cell \"different words\"\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NExt STEPs:\n",
    "\n",
    "- go through dataset and see which words differ\n",
    "- get dependent words through pos tags\n",
    "- run these words through BERT\n",
    "- get word embeddings as output\n",
    "- compute vector distance\n",
    "- whichever distance is lower is better\n",
    "- output is a measure of the difference of the distances\n",
    "\n",
    "All of this (without first, maybe second step) cann happen in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
