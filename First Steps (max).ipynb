{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in Computational Linguistics\n",
    "## SemEval 2020: Commonsense Validation and Explanation\n",
    "\n",
    "We're participating in task 4 of the SemEval 2020 Challenges for our seminar Challenges in Computational Linguistics, University TÃ¼bingen.\n",
    "\n",
    "This notebook is meant as playground and first steps, to get the ball rolling. It can later be used as a template to build the final notebook (or program)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "### Read-in for task 1\n",
    "\n",
    "First I use the given data from the tasks github respo for subtask A.\n",
    "\n",
    "https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/tree/master/Training%20%20Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports for data\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He poured orange juice on his cereal.</td>\n",
       "      <td>He poured milk on his cereal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He drinks apple.</td>\n",
       "      <td>He drinks milk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeff ran a mile today</td>\n",
       "      <td>Jeff ran 100,000 miles today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    sent0                          sent1\n",
       "id                                                                      \n",
       "0   He poured orange juice on his cereal.  He poured milk on his cereal.\n",
       "1                        He drinks apple.                He drinks milk.\n",
       "2                   Jeff ran a mile today   Jeff ran 100,000 miles today"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Read in the data directly from github\"\"\"\n",
    "url_data_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv\"\n",
    "url_answers_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv\"\n",
    "\n",
    "data_task_A = pd.read_csv(url_data_task_A,header=0, index_col=0)\n",
    "answers_task_A = pd.read_csv(url_answers_task_A, index_col=0)\n",
    "\n",
    "data_task_A[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> shape of data: (10000, 2) shape of answers: (9999, 1) one line is missing \n",
      "because no header here\n",
      "\n",
      "To get first column, first row: He poured orange juice on his cereal.\n",
      "\n",
      "To get both colums for given row: sent0    He poured orange juice on his cereal.\n",
      "sent1            He poured milk on his cereal.\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check data type, shape, etc\n",
    "print(type(data_task_A), 'shape of data:',data_task_A.shape, 'shape of answers:',\n",
    "      answers_task_A.shape, 'one line is missing \\nbecause no header here\\n')\n",
    "\n",
    "print('To get first column, first row:', data_task_A['sent0'].iloc[0]) # iloc only takes integers\n",
    "print('\\nTo get both colums for given row:',data_task_A.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be enough in respect to the data for now, next step is manipuating the data to our needs.\n",
    "\n",
    "## Using spacy for Natural Language Processing\n",
    "### What about NLTK ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now the fun starts...\"\"\"\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/max/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking and loading stuff...\n",
    "nltk.__version__\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenized DataFrame\n",
    "\n",
    "With help of Pythons list comprehension, we're transforming the string sentences into list of tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sentence=pd.DataFrame([[nltk.word_tokenize(row['sent0']), nltk.word_tokenize(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[He, poured, orange, juice, on, his, cereal, .]</td>\n",
       "      <td>[He, poured, milk, on, his, cereal, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[He, drinks, apple, .]</td>\n",
       "      <td>[He, drinks, milk, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Jeff, ran, a, mile, today]</td>\n",
       "      <td>[Jeff, ran, 100,000, miles, today]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sent0  \\\n",
       "0  [He, poured, orange, juice, on, his, cereal, .]   \n",
       "1                           [He, drinks, apple, .]   \n",
       "2                      [Jeff, ran, a, mile, today]   \n",
       "\n",
       "                                    sent1  \n",
       "0  [He, poured, milk, on, his, cereal, .]  \n",
       "1                   [He, drinks, milk, .]  \n",
       "2      [Jeff, ran, 100,000, miles, today]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of distinct words\n",
    "\n",
    "This could be done together with the above tokenization inside one loop, for better readability, I separated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_dist_words = nltk.FreqDist()\n",
    "\n",
    "# there should be a way to skip the double lookup...\n",
    "for i, rows in tokens_per_sentence.iterrows():\n",
    "    for word1, word2 in zip(rows['sent0'], rows['sent1']):\n",
    "        \n",
    "        # if it's the same word, we don't want to count it as double\n",
    "        if word1.lower() == word2.lower():\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "        else:\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "            number_dist_words[word2.lower()] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct words: 8078\n",
      "Output first 10 words:\n",
      "\n",
      "\"he\" occures 1782 number of times.\n",
      "\"poured\" occures 20 number of times.\n",
      "\"orange\" occures 17 number of times.\n",
      "\"milk\" occures 110 number of times.\n",
      "\"juice\" occures 26 number of times.\n",
      "\"on\" occures 1098 number of times.\n",
      "\"his\" occures 859 number of times.\n",
      "\"cereal\" occures 11 number of times.\n",
      "\".\" occures 3560 number of times.\n",
      "\"drinks\" occures 40 number of times.\n"
     ]
    }
   ],
   "source": [
    "print('number of distinct words:', len(number_dist_words))\n",
    "print('Output first 10 words:\\n')\n",
    "i = 0\n",
    "for key, val in number_dist_words.items():\n",
    "    print('\"{}\"'.format(key), 'occures', val, 'number of times.')\n",
    "    i += 1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagger\n",
    "\n",
    "A quick implementation of a part-of-speech-tagger. This again could be done inside the above forloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/max/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_per_sentence = pd.DataFrame([[nltk.pos_tag(row['sent0']), nltk.pos_tag(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...</td>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent0  \\\n",
       "0  [(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...   \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...   \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...   \n",
       "\n",
       "                                               sent1  \n",
       "0  [(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...  \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...  \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "### BllipParser\n",
    "\n",
    "\n",
    "Let's use, for simplicity purposes, the bllip parser module that comes with the nltk package.\n",
    "\n",
    "Sadly, this didn't work in the beginning, so I quickly downloaded the bllipparser directly from their github:\n",
    "https://github.com/BLLIP/bllip-parser and\n",
    "https://github.com/BLLIP/bllip-parser/blob/master/README-python.rst\n",
    "\n",
    "If you want to try the next few lines, make sure to follow the steps and install the package and WSJ+Gigaword-v2 Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.bllip import BllipParser # this doesn't work: ImportError: Couldn't import bllipparser module: No module named 'bllipparser'\n",
    "from bllipparser import RerankingParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parser is already loaded and can only be loaded once.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f34349fedc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBllipParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/max/.local/share/bllipparser/WSJ+Gigaword-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/parse/bllip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser_model, reranker_features, reranker_weights, parser_options, reranker_options)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRerankingParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parser_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparser_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreranker_features\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreranker_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.rrp.load_reranker_model(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/bllipparser/RerankingParser.py\u001b[0m in \u001b[0;36mload_parser_model\u001b[0;34m(self, model_dir, terms_only, heads_only, **parser_options)\u001b[0m\n\u001b[1;32m    574\u001b[0m         be loaded but the full parsing model will not.\"\"\"\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRerankingParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser_model_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             raise RuntimeError('Parser is already loaded and can only '\n\u001b[0m\u001b[1;32m    577\u001b[0m                                'be loaded once.')\n\u001b[1;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_path_or_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Parser model directory'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parser is already loaded and can only be loaded once."
     ]
    }
   ],
   "source": [
    "nltk_parser = BllipParser('/Users/max/.local/share/bllipparser/WSJ+Gigaword-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_path = '/Users/max/.local/share/bllipparser/WSJ+Gigaword-v2'\n",
    "\n",
    "\"\"\"This is a time consuming operation (1min)\"\"\"\n",
    "parser = RerankingParser.from_unified_model_dir(parser_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'En', 'case_insensitive': False, 'nbest': 2, 'small_corpus': True, 'overparsing': 21, 'debug': 0, 'smooth_pos': 0}\n",
      "I put an elephant in the fridge\n",
      "2 x\n",
      "-15.839081581410 -63.140991134783\n",
      "(S1 (S (NP (PRP I)) (VP (VBP put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "-16.692901096060 -65.869600587365\n",
      "(S1 (S (NP (PRP I)) (VP (VB put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "\n",
      "I put a turkey in the fridge\n",
      "2 x\n",
      "-15.839081581410 -63.140991134783\n",
      "(S1 (S (NP (PRP I)) (VP (VBP put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "-16.692901096060 -65.869600587365\n",
      "(S1 (S (NP (PRP I)) (VP (VB put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "parser.parse outputs a list of N most probable parses, default n = 50\n",
    "parser.set_parser_options(nbest=10) sets new options and outputs a dict with current ones\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parser_options = parser.set_parser_options(nbest=2)\n",
    "print(parser_options)\n",
    "\n",
    "test_a = 'I put an elephant in the fridge'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "parsed_sen_a = parser.parse(test_a)\n",
    "parsed_sen_b = parser.parse(test_b)\n",
    "print(test_a)\n",
    "print(parsed_sen_b)\n",
    "print(test_b)\n",
    "print(parsed_sen_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary BllipParser\n",
    "As we can see, we get the same scores for both sentences. This parser is useful for getting the dependencies of the tokens, or rather the grammatical structure.\n",
    "\n",
    "While this can certainly help us, we additionally want a parser that computs the propability P of a word W2, given a W1: P(W2|W1). Then We can compute P(S) for sentence S = W1 W2 W3 as P(S)=P(W1|start)+P(W2|W1)+P(W3|W2)+P(end|W3)\n",
    "\n",
    "BTW; I'm not sure on that probability calculation above, i think it differs for depenendent/independent events, so we have to check that again.\n",
    "\n",
    "We can probably make more use of the sentence structure for task B\n",
    "\n",
    "### StanfordParser\n",
    "\n",
    "As suggested by Cagri, we could also make use of the StanfordParser. It usually runs on Java, so we have to make some adjustments to our python script if we want to use it. I found couple good-looking lines from: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "Also on the official site https://nlp.stanford.edu/software/lex-parser.shtml#Download an interface to python is linked:http://projects.csail.mit.edu/spatial/Stanford_Parser which I have yet to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT an) (NN elephant))\n",
      "      (PP (IN in) (NP (DT the) (NN fridge))))))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "java_path = r'/Library/Java/JavaVirtualMachines/jdk-10.jdk/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "stanford_parser = StanfordParser(path_to_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "# we again use test sentences test_a & test_b from above\n",
    "result = list(stanford_parser.raw_parse(test_a))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try out the CoreNLPParser as suggested in the DeprecationWarning above.\n",
    "\n",
    "I tried drawing the tree like the tutorial linked above suggsted, but ran into some dependency-problems which I wont delve into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT an) (NN elephant))\n",
      "      (PP (IN in) (NP (DT the) (NN fridge))))))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"StanfordDependencyParser\"\"\"\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "depend_parser = StanfordParser(path_to_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "result = list(depend_parser.raw_parse(test_a))  \n",
    "print(result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary StanfordParser\n",
    "\n",
    "Seems like both run on the CoreNLPParser, so we should look into this one. Also we should research if these Parser can also output the propability we actually want out of our parses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
