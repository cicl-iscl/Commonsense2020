{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in Computational Linguistics\n",
    "## SemEval 2020: Commonsense Validation and Explanation\n",
    "\n",
    "We're participating in task 4 of the SemEval 2020 Challenges for our seminar Challenges in Computational Linguistics, University Tübingen.\n",
    "\n",
    "This notebook is meant as playground and first steps, to get the ball rolling. It can later be used as a template to build the final notebook (or program)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "### Read-in for task 1\n",
    "\n",
    "First I use the given data from the tasks github respo for subtask A.\n",
    "\n",
    "https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/tree/master/Training%20%20Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports for data\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    sent0                          sent1\n",
      "id                                                                      \n",
      "0   He poured orange juice on his cereal.  He poured milk on his cereal.\n",
      "1                        He drinks apple.                He drinks milk.\n",
      "2                   Jeff ran a mile today   Jeff ran 100,000 miles today\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0   \n",
       "0  0\n",
       "1  0\n",
       "2  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Read in the data directly from github\"\"\"\n",
    "url_data_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv\"\n",
    "url_answers_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv\"\n",
    "\n",
    "data_task_A = pd.read_csv(url_data_task_A,header=0, index_col=0)\n",
    "answers_task_A = pd.read_csv(url_answers_task_A, index_col=0, header=None)\n",
    "\n",
    "print(data_task_A[:3])\n",
    "answers_task_A[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> shape of data: (10000, 2) shape of answers: (9999, 1) one line is missing \n",
      "because no header here\n",
      "\n",
      "To get first column, first row: He poured orange juice on his cereal.\n",
      "\n",
      "To get both colums for given row: sent0    He poured orange juice on his cereal.\n",
      "sent1            He poured milk on his cereal.\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check data type, shape, etc\n",
    "print(type(data_task_A), 'shape of data:',data_task_A.shape, 'shape of answers:',\n",
    "      answers_task_A.shape, 'one line is missing \\nbecause no header here\\n')\n",
    "\n",
    "print('To get first column, first row:', data_task_A['sent0'].iloc[0]) # iloc only takes integers\n",
    "print('\\nTo get both colums for given row:',data_task_A.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Using spacy for Natural Language Processing\n",
    "### What about NLTK ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now the fun starts...\"\"\"\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/max/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking and loading stuff...\n",
    "nltk.__version__\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenized DataFrame\n",
    "\n",
    "With help of Pythons list comprehension, we're transforming the string sentences into list of tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sentence=pd.DataFrame([[nltk.word_tokenize(row['sent0']), nltk.word_tokenize(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[He, poured, orange, juice, on, his, cereal, .]</td>\n",
       "      <td>[He, poured, milk, on, his, cereal, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[He, drinks, apple, .]</td>\n",
       "      <td>[He, drinks, milk, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Jeff, ran, a, mile, today]</td>\n",
       "      <td>[Jeff, ran, 100,000, miles, today]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sent0  \\\n",
       "0  [He, poured, orange, juice, on, his, cereal, .]   \n",
       "1                           [He, drinks, apple, .]   \n",
       "2                      [Jeff, ran, a, mile, today]   \n",
       "\n",
       "                                    sent1  \n",
       "0  [He, poured, milk, on, his, cereal, .]  \n",
       "1                   [He, drinks, milk, .]  \n",
       "2      [Jeff, ran, 100,000, miles, today]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of distinct words\n",
    "\n",
    "This could be done together with the above tokenization inside one loop, for better readability, I separated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_dist_words = nltk.FreqDist()\n",
    "\n",
    "# there should be a way to skip the double lookup...\n",
    "for i, rows in tokens_per_sentence.iterrows():\n",
    "    for word1, word2 in zip(rows['sent0'], rows['sent1']):\n",
    "        \n",
    "        # if it's the same word, we don't want to count it as double\n",
    "        if word1.lower() == word2.lower():\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "        else:\n",
    "            number_dist_words[word1.lower()] += 1\n",
    "            number_dist_words[word2.lower()] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct words: 8078\n",
      "Output first 10 words:\n",
      "\n",
      "\"he\" occures 1782 number of times.\n",
      "\"poured\" occures 20 number of times.\n",
      "\"orange\" occures 17 number of times.\n",
      "\"milk\" occures 110 number of times.\n",
      "\"juice\" occures 26 number of times.\n",
      "\"on\" occures 1098 number of times.\n",
      "\"his\" occures 859 number of times.\n",
      "\"cereal\" occures 11 number of times.\n",
      "\".\" occures 3560 number of times.\n",
      "\"drinks\" occures 40 number of times.\n"
     ]
    }
   ],
   "source": [
    "print('number of distinct words:', len(number_dist_words))\n",
    "print('Output first 10 words:\\n')\n",
    "i = 0\n",
    "for key, val in number_dist_words.items():\n",
    "    print('\"{}\"'.format(key), 'occures', val, 'number of times.')\n",
    "    i += 1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagger\n",
    "\n",
    "A quick implementation of a part-of-speech-tagger. This again could be done inside the above forloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/max/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_per_sentence = pd.DataFrame([[nltk.pos_tag(row['sent0']), nltk.pos_tag(row['sent1'])\n",
    "                                  ] for i, row in data_task_A.iterrows()], columns=['sent0','sent1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...</td>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "      <td>[(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "      <td>[(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent0  \\\n",
       "0  [(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...   \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...   \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...   \n",
       "\n",
       "                                               sent1  \n",
       "0  [(H, NNP), (e, NN), ( , NNP), (p, NN), (o, NN)...  \n",
       "1  [(H, NNP), (e, NN), ( , NNP), (d, NN), (r, NN)...  \n",
       "2  [(J, NNP), (e, NN), (f, NN), (f, NN), ( , NNP)...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_per_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "### BllipParser\n",
    "\n",
    "\n",
    "Let's use, for simplicity purposes, the bllip parser module that comes with the nltk package.\n",
    "\n",
    "Sadly, this didn't work in the beginning, so I quickly downloaded the bllipparser directly from their github:\n",
    "https://github.com/BLLIP/bllip-parser and\n",
    "https://github.com/BLLIP/bllip-parser/blob/master/README-python.rst\n",
    "\n",
    "If you want to try the next few lines, make sure to follow the steps and install the package and WSJ+Gigaword-v2 Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.bllip import BllipParser # this doesn't work: ImportError: Couldn't import bllipparser module: No module named 'bllipparser'\n",
    "from bllipparser import RerankingParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_parser = BllipParser('/Users/max/.local/share/bllipparser/WSJ+Gigaword-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_path = '/Users/max/.local/share/bllipparser/WSJ+Gigaword-v2'\n",
    "\n",
    "\"\"\"This is a time consuming operation (1min)\"\"\"\n",
    "parser = RerankingParser.from_unified_model_dir(parser_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'En', 'case_insensitive': False, 'nbest': 2, 'small_corpus': True, 'overparsing': 21, 'debug': 0, 'smooth_pos': 0}\n",
      "I put an elephant in the fridge\n",
      "2 x\n",
      "-15.839081581410 -63.140991134783\n",
      "(S1 (S (NP (PRP I)) (VP (VBP put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "-16.692901096060 -65.869600587365\n",
      "(S1 (S (NP (PRP I)) (VP (VB put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "\n",
      "I put a turkey in the fridge\n",
      "2 x\n",
      "-15.839081581410 -63.140991134783\n",
      "(S1 (S (NP (PRP I)) (VP (VBP put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "-16.692901096060 -65.869600587365\n",
      "(S1 (S (NP (PRP I)) (VP (VB put) (NP (DT a) (NN turkey)) (PP (IN in) (NP (DT the) (NN fridge))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "parser.parse outputs a list of N most probable parses, default n = 50\n",
    "parser.set_parser_options(nbest=10) sets new options and outputs a dict with current ones\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parser_options = parser.set_parser_options(nbest=2)\n",
    "print(parser_options)\n",
    "\n",
    "test_a = 'I put an elephant in the fridge'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "parsed_sen_a = parser.parse(test_a)\n",
    "parsed_sen_b = parser.parse(test_b)\n",
    "print(test_a)\n",
    "print(parsed_sen_b)\n",
    "print(test_b)\n",
    "print(parsed_sen_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary BllipParser\n",
    "As we can see, we get the same scores for both sentences. This parser is useful for getting the dependencies of the tokens, or rather the grammatical structure.\n",
    "\n",
    "While this can certainly help us, we additionally want a parser that computs the propability P of a word W2, given a W1: P(W2|W1). Then We can compute P(S) for sentence S = W1 W2 W3 as P(S)=P(W1|start)+P(W2|W1)+P(W3|W2)+P(end|W3)\n",
    "\n",
    "BTW; I'm not sure on that probability calculation above, i think it differs for depenendent/independent events, so we have to check that again.\n",
    "\n",
    "We can probably make more use of the sentence structure for task B\n",
    "\n",
    "### StanfordParser\n",
    "\n",
    "As suggested by Cagri, we could also make use of the StanfordParser. It usually runs on Java, so we have to make some adjustments to our python script if we want to use it. I found couple good-looking lines from: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "Also on the official site https://nlp.stanford.edu/software/lex-parser.shtml#Download an interface to python is linked:http://projects.csail.mit.edu/spatial/Stanford_Parser which I have yet to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT an) (NN elephant))\n",
      "      (PP (IN in) (NP (DT the) (NN fridge))))))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "java_path = r'/Library/Java/JavaVirtualMachines/jdk-10.jdk/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "stanford_parser = StanfordParser(path_to_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "# we again use test sentences test_a & test_b from above\n",
    "result = list(stanford_parser.raw_parse(test_a))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try out the CoreNLPParser as suggested in the DeprecationWarning above.\n",
    "\n",
    "I tried drawing the tree like the tutorial linked above suggsted, but ran into some dependency-problems which I wont delve into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.Tree'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"StanfordDependencyParser\"\"\"\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "depend_parser = StanfordParser(path_to_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='/Users/max/Documents/GitHub/Commonsense2020/Parsers/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "result = list(depend_parser.raw_parse(test_a))  \n",
    "print(result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary StanfordParser\n",
    "\n",
    "Seems like both run on the CoreNLPParser, so we should look into this one. Also we should research if these Parser can also output the propability we actually want out of our parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "The thing always going through my mind with a propability calculation of the whole sentence is called a Language Model (should've known that) and spacy provides us with a pretrained one...\n",
    "\n",
    "Alright, following I'm trying to implement the BERT model to compute word embeddings for three words out of our dataset. Pipeline will be as follows:\n",
    "\n",
    "- go through dataset and see which words differ\n",
    "- get dependent words through pos tags\n",
    "- run these words through BERT\n",
    "- get word embeddings as output\n",
    "- compute vector distance\n",
    "- whichever distance is lower is better\n",
    "- output is a measure of the difference of the distances\n",
    "\n",
    "All of this (without first, maybe second step) cann happen in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://github.com/huggingface/transformers\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "# FullTokenizer from file tokenization.py, method copied from bert-github:https://github.com/google-research/bert\n",
    "# with tensorflow 2, the normal implementation doesnt work anymore, you have to run the upgrade script on the file.\n",
    "import tokenization_v2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different words\n",
    "We want some more information about our data like\n",
    "- longest word sequence\n",
    "- word pairs that differ between the sentences\n",
    "- dependents on the different words (looking at POS-tags should we a way to find them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1209 20:09:36.517829 4591455680 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/max/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1209 20:09:37.102277 4591455680 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/max/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1209 20:09:37.107390 4591455680 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1209 20:09:37.572799 4591455680 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 not found in cache or force_download set to True, downloading to /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tmpd9_sb5xm\n",
      "100%|██████████| 536063208/536063208 [01:18<00:00, 6863807.50B/s]\n",
      "I1209 20:10:56.243159 4591455680 file_utils.py:309] copying /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tmpd9_sb5xm to cache at /Users/max/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I1209 20:10:57.762202 4591455680 file_utils.py:313] creating metadata file for /Users/max/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I1209 20:10:57.763890 4591455680 file_utils.py:322] removing temp file /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tmpd9_sb5xm\n",
      "I1209 20:10:57.817735 4591455680 modeling_tf_utils.py:258] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /Users/max/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights =  TFBertModel, BertTokenizer, 'bert-base-uncased'\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'poured', 'orange', 'juice', 'on', 'his', 'cereal', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so that apparently worked...\n",
    "\n",
    "test_a = 'I put an elephant in the fridge'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "toks_a = tokenizer.tokenize('He poured orange juice on his cereal.')\n",
    "toks_a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['[CLS]', 'he', 'poured', 'orange', 'juice', 'on', 'his', 'cereal', '.', '[SEP]'], ['[CLS]', 'he', 'poured', 'milk', 'on', 'his', 'cereal', '.', '[SEP]']], [['[CLS]', 'he', 'drinks', 'apple', '.', '[SEP]'], ['[CLS]', 'he', 'drinks', 'milk', '.', '[SEP]']], [['[CLS]', 'jeff', 'ran', 'a', 'mile', 'today', '[SEP]'], ['[CLS]', 'jeff', 'ran', '100', ',', '000', 'miles', 'today', '[SEP]']]] \n",
      "Longest sentence length: 27 \n",
      "First 5 word-pairs that differ: [[list(['orange', 'juice']) list(['milk'])]\n",
      " [list(['apple']) list(['milk'])]\n",
      " [list(['a', 'mile']) list(['100', ',', '000', 'miles'])]\n",
      " [list(['##s', 'me']) list(['i'])]\n",
      " [list(['niece']) list(['gi', '##raf', '##fe'])]] \n",
      "shape of different_words: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"DISCLAIMER: Right now we use nltk tokenizer in oder to get whole words, but not things like [gi, ##ra, ##fe]\"\"\"\n",
    "\n",
    "\"\"\"We use the official FullTokenizer just to make sure\"\"\"\n",
    "tokenizer = tokenization_v2.FullTokenizer('BERT/vocab.txt')\n",
    "sen_tok = tokenizer.tokenize(test_a)\n",
    "\n",
    "toks_task_A = []\n",
    "max_seq_len = 0\n",
    "different_words = np.asarray([[None,None]] * (data_task_A.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "# data_task_A, answers_task_A\n",
    "for index, row in data_task_A.iterrows():\n",
    "    \n",
    "    sen_0_tok = tokenizer.tokenize(row['sent0'])\n",
    "    sen_1_tok = tokenizer.tokenize(row['sent1'])\n",
    "    \n",
    "    # here we test another tokenizer, from nltk (implementation some cell above...)\n",
    "    # outcomment sen_0_tok and sen_1_tok above according to which tokenizer is required.\n",
    "    #sen_0_tok = nltk.word_tokenize(row['sent0'])\n",
    "    #sen_1_tok = nltk.word_tokenize(row['sent1'])\n",
    "\n",
    "    len_sen_0_tok = len(sen_0_tok)\n",
    "    len_sen_1_tok = len(sen_1_tok)\n",
    "    \n",
    "    \"\"\"Here we get the longest token-sequence length\"\"\"\n",
    "    if len_sen_0_tok >= len_sen_1_tok:\n",
    "        if len_sen_0_tok > max_seq_len:\n",
    "            max_seq_len = len_sen_0_tok\n",
    "    elif len_sen_1_tok > max_seq_len:\n",
    "            max_seq_len = len_sen_1_tok\n",
    "    \n",
    "    \"\"\"Here we look for the words that differ\"\"\"\n",
    "    if len_sen_0_tok == len_sen_1_tok:\n",
    "        for tok_0, tok_1 in zip(sen_0_tok, sen_1_tok):\n",
    "            if tok_0 != tok_1:\n",
    "\n",
    "                different_words[index][0] = [tok_0]\n",
    "                different_words[index][1] = [tok_1]\n",
    "                \n",
    "    # We should make sure that the different words stay in the spots for sent0 sent1,\n",
    "    else:\n",
    "        \n",
    "        if len_sen_0_tok > len_sen_1_tok:\n",
    "            different_words[index][0] = [x for x in sen_0_tok if x not in sen_1_tok]\n",
    "            different_words[index][1] = [x for x in sen_1_tok if x not in sen_0_tok]\n",
    "        else:\n",
    "            different_words[index][0] = [x for x in sen_0_tok if x not in sen_1_tok]\n",
    "            different_words[index][1] = [x for x in sen_1_tok if x not in sen_0_tok]\n",
    "            \n",
    "        \n",
    "    sen_0_tok.insert(0,'[CLS]')\n",
    "    sen_0_tok.append('[SEP]')\n",
    "    sen_1_tok.insert(0, '[CLS]')\n",
    "    sen_1_tok.append('[SEP]')\n",
    "    \n",
    "    toks_task_A.append([sen_0_tok,sen_1_tok])\n",
    "\n",
    "# add 2 for tokens CLS and SEP to max length\n",
    "max_seq_len += 2\n",
    "\n",
    "\n",
    "# just checking...\n",
    "print(toks_task_A[:3], '\\nLongest sentence length:', max_seq_len, \n",
    "      '\\nFirst 5 word-pairs that differ:', different_words[:5],\n",
    "      '\\nshape of different_words:', np.asarray(different_words).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the dependants of different_words, but how...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saver below loads the graph-operations of pretrained model.\n",
    "First we need to load the meta graph, then the weights.\n",
    "\"\"\"\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.import_meta_graph('BERT/bert_model.ckpt.meta')\n",
    "saver.restore(sess, 'BERT/bert_model.ckpt')\n",
    "\n",
    "\"\"\"\n",
    "the saver imports the graph of the saved model and with restore, loads the weights into the different operations.\n",
    "So if we want to load the model from file and do a classification with it, we need to modify the graph.\n",
    "Precisely, we need to add 3 Input layers (operations) like two cells below from: https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
    "Also a classification ops like https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with tensorflow hub\n",
    "\n",
    "Because working directly with the graph and operations can be quite tiresome, we'll try tensorflow hub from https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22. This promises us quicker results. Ill still want to figure out how to work with the graph later in the game.\n",
    "\n",
    "Also this tutorial works with tensorflow 2, so make sure you have that installed.\n",
    "Check with tensorflow.\\__version__\n",
    "\n",
    "In the final vesion, we want to only use the dependent words of the different words. For now, we'll just use all of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1209 21:18:00.876294 4591455680 resolver.py:79] Using /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tfhub_modules to cache modules.\n",
      "I1209 21:18:00.881837 4591455680 resolver.py:400] Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'.\n",
      "I1209 21:18:17.061730 4591455680 resolver.py:122] Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 120.00MB\n",
      "I1209 21:18:32.984575 4591455680 resolver.py:122] Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 240.00MB\n",
      "I1209 21:18:48.356165 4591455680 resolver.py:122] Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 350.00MB\n",
      "I1209 21:18:57.544800 4591455680 resolver.py:122] Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1, Total size: 423.26MB\n",
      "I1209 21:18:57.547782 4591455680 resolver.py:415] Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = max_seq_len  # We take the max length from above cell \"different words\"\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 109,482,241\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again methods from: https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Token length more than max seq length!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-87744b8ae257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0minput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minput_segments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-2412f7cb4ec1>\u001b[0m in \u001b[0;36mget_masks\u001b[0;34m(tokens, max_seq_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"Mask for padding\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Token length more than max seq length!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Token length more than max seq length!"
     ]
    }
   ],
   "source": [
    "# This takes some time... \n",
    "import time\n",
    "\n",
    "embs_task_A = np.asarray([[None,None]] * (data_task_A.shape[0]))\n",
    "start = time.time()\n",
    "for row, sentence_pair in enumerate(toks_task_A):\n",
    "    #print( sentence_pair)\n",
    "    for i, sen in enumerate(sentence_pair):\n",
    "        #print(sen)\n",
    "        stokens = sen\n",
    "        input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "        input_masks = get_masks(stokens, max_seq_length)\n",
    "        input_segments = get_segments(stokens, max_seq_length)\n",
    "\n",
    "        pool_embs, _ = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "        embs_task_A[row][i] = pool_embs\n",
    "\n",
    "end = time.time()\n",
    "print('Building the word embeddings took...', end-start)\n",
    "print(embs_task_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SAVE THEM AS NPY ARRAYS!!!\"\"\"\n",
    "import numpy as np\n",
    "npy_task_A = embs_task_A\n",
    "np.save('sentence_embeddings.npy', npy_task_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768) (1, 25, 768)\n"
     ]
    }
   ],
   "source": [
    "print(pool_embs.shape, all_embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NExt STEPs:\n",
    "\n",
    "- go through dataset and see which words differ\n",
    "- get dependent words through pos tags\n",
    "- run these words through BERT\n",
    "- get word embeddings as output\n",
    "- compute vector distance\n",
    "- whichever distance is lower is better\n",
    "- output is a measure of the difference of the distances\n",
    "\n",
    "All of this (without first, maybe second step) cann happen in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1205 12:23:18.874933 4558089664 tokenization_openai.py:141] ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133.0363788241483, 13605.462201234779]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "model.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss=model(tensor_input, lm_labels=tensor_input)\n",
    "    return math.exp(loss)\n",
    "\n",
    "\n",
    "a=['I put a turkey into the fridge', 'I put a elephant the fridge']\n",
    "print([score(i) for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[385.0906700452085, 738.7083743906966]\n",
      "[13233.921159142094, 4031.916840866506]\n",
      "[1270.5987178679527, 1946.7255548497253]\n",
      "[722.3442951187566, 320.8368294456649]\n"
     ]
    }
   ],
   "source": [
    "a=['Pouring milk on my cereal', 'Pouring orangejuice on my cereal']\n",
    "print([score(i) for i in a])\n",
    "a=['I drink apple', 'I drink milk']\n",
    "print([score(i) for i in a])\n",
    "a=['John ran a mile yesterday', 'John ran 100.000 miles yesterday']\n",
    "print([score(i) for i in a])\n",
    "a=['Hold my tits', 'Hold my beer']\n",
    "print([score(i) for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
