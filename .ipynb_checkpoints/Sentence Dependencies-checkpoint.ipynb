{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Dependencies\n",
    "\n",
    "Ok, so a big part in getting better scores will be to narrow down the problem of the sentence pairs, for this we will need to know important words and their dependents. This notebook will serve solely to work out methods how to get those, so the other ones are not overloaded with stuff (like First Steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
      "\u001b[K     |████████████████████████████████| 827.9MB 830kB/s eta 0:00:012:11:03     |██▌                             | 65.9MB 580kB/s eta 0:21:53     |████▏                           | 107.3MB 382kB/s eta 0:31:23     |██████                          | 155.3MB 259kB/s eta 0:43:10     |█████████                       | 233.1MB 382kB/s eta 0:25:55     |███████████████                 | 388.6MB 569kB/s eta 0:12:51     |█████████████████▌              | 451.7MB 1.9MB/s eta 0:03:14     |██████████████████▌             | 478.7MB 590kB/s eta 0:09:52     |███████████████████▏            | 496.7MB 496kB/s eta 0:11:07     |████████████████████▏           | 521.8MB 688kB/s eta 0:07:25     |█████████████████████▎          | 551.4MB 719kB/s eta 0:06:25     |█████████████████████▌          | 557.1MB 920kB/s eta 0:04:54     |██████████████████████          | 566.9MB 749kB/s eta 0:05:49     |██████████████████████          | 569.2MB 720kB/s eta 0:06:00     |██████████████████████▋         | 585.4MB 573kB/s eta 0:07:03| 594.1MB 164kB/s eta 0:23:45     |███████████████████████▌        | 607.2MB 3.0MB/s eta 0:01:14     |████████████████████████▌       | 633.9MB 767kB/s eta 0:04:13     |████████████████████████▌       | 634.8MB 198kB/s eta 0:16:14     |████████████████████████████▏   | 728.8MB 751kB/s eta 0:02:12\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/site-packages (from en_core_web_lg==2.2.5) (2.2.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (41.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.16.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.23)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.38.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.2)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.2->en_core_web_lg==2.2.5) (7.2.0)\n",
      "Building wheels for collected packages: en-core-web-lg\n",
      "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180945 sha256=fa9431f0487c187d17dfb448fe02499da55ca1824fa4326a1736c8b589c68d50\n",
      "  Stored in directory: /private/var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/pip-ephem-wheel-cache-s8ydbczv/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
      "Successfully built en-core-web-lg\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "I put a turkey in the fridge\n",
      "A DET det knife NOUN []\n",
      "knife NOUN nsubj is AUX [A]\n",
      "is AUX ROOT is AUX [knife, instrument]\n",
      "an DET det instrument NOUN []\n",
      "instrument NOUN attr is AUX [an, composed]\n",
      "composed VERB acl instrument NOUN [of]\n",
      "of ADP prep composed VERB [blade]\n",
      "a DET det blade NOUN []\n",
      "blade NOUN pobj of ADP [a, fixed]\n",
      "fixed VERB acl blade NOUN [into]\n",
      "into ADP prep fixed VERB [handle]\n",
      "a DET det handle NOUN []\n",
      "handle NOUN pobj into ADP [a, used]\n",
      "used VERB acl handle NOUN [for]\n",
      "for ADP prep used VERB [cutting]\n",
      "cutting VERB pcomp for ADP []\n",
      "\n",
      "\n",
      "\n",
      "I nsubj put VERB []\n",
      "put ROOT put VERB [I, turkey, in]\n",
      "a det turkey NOUN []\n",
      "turkey dobj put VERB [a]\n",
      "in prep put VERB [fridge]\n",
      "the det fridge NOUN []\n",
      "fridge pobj in ADP [the]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#parser = spacy.load('en_core_web_sm')\n",
    "parser = spacy.load('en_core_web_lg')\n",
    "\n",
    "#test_a = 'I put a elephant in the fridge'\n",
    "test_a = 'A knife is an instrument composed of a blade fixed into a handle used for cutting'\n",
    "test_b = 'I put a turkey in the fridge'\n",
    "\n",
    "# for this we want to remove stop words, to be implemented\n",
    "different_words = ['elephant', 'turkey']\n",
    "different_words_idx = [3,4]\n",
    "\n",
    "parse_a = parser(test_a)\n",
    "parse_b = parser(test_b)\n",
    "print(type(parse_a))\n",
    "print(parse_b)\n",
    "for token in parse_a:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n",
    "print('\\n\\n')\n",
    "for token in parse_b:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THis will be methof get_dependents\"\"\"\n",
    "\n",
    "\"\"\"PYTHONIC: maybe it would be cleaner to only do one sentence at a time,\n",
    "and handle the structure of the sentences further up in the logic\"\"\"\n",
    "\n",
    "####\n",
    "# TODO: NEGATIONS AND CONDITIONS, LOOK AT SEMANTIC PARSING SEMPRE (more likely for task b)\n",
    "# ALSO USE BERT WITH SPACY!\n",
    "####\n",
    "\n",
    "#print(type(parse_a))\n",
    "def get_dependents(parser, different_words, different_words_idx, sen):\n",
    "    parse = parser(sen)\n",
    "    verb_dependents = []\n",
    "    #print(parse, different_words)\n",
    "    #print(parse)\n",
    "    #for w, idx in zip(different_words, different_words_idx):\n",
    "    for w in different_words.split():\n",
    "        \n",
    "        w = parser(w)\n",
    "        ####\n",
    "        # Case 1: Different word is a verb TOBETESTED\n",
    "        # add verb to dependents, dont go to Case 2 that doesnt fucking work\n",
    "        ###\n",
    "        #for ele in w:\n",
    "         #   if ele.pos_ == 'VERB':\n",
    "          #      verb_dependents.append(ele)\n",
    "        \n",
    "        # convert to string\n",
    "        w = w.text\n",
    "        #this if is necessary because my fucked up data\n",
    "        if w == ' ':\n",
    "            continue\n",
    "        idx = get_dependent_index(parse, w)\n",
    "        if idx == None:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        ####\n",
    "        # Case 2: Different words head is verb\n",
    "        # get subj, objs of verb and add to dependents\n",
    "        ####\n",
    "        if w in parse.text:\n",
    "            parent = parse[idx].head\n",
    "            #print(parent)\n",
    "            verb_dependents = []\n",
    "            # first we check what pos tags our parent has, in order to build a decision tree...\n",
    "            if parent.pos_ == 'VERB':\n",
    "                # now we want all children of the verb with subj and obj labels\n",
    "                \n",
    "                for child in parent.children:\n",
    "                    if child.dep_[-2:] == 'bj':\n",
    "                        verb_dependents.append(child)\n",
    "                    else:\n",
    "                        for nephew in child.children:\n",
    "                            if nephew.dep_[-2:] == 'bj':\n",
    "                                verb_dependents.append(nephew)\n",
    "    \n",
    "    try:\n",
    "        return verb_dependents\n",
    "    except: return 0\n",
    "    \"\"\"                    \n",
    "    elif w in parse_b.text:\n",
    "        parent = parse_b[idx].head\n",
    "        \n",
    "        # first we check what pos tags our parent has, in order to build a decision tree...\n",
    "        if parent.pos_ == 'VERB':\n",
    "            # now we want all children of the verb with subj and obj labels\n",
    "            verb_dependents_b = []\n",
    "            for child in parent.children:\n",
    "                if child.dep_[-2:] == 'bj':\n",
    "                    verb_dependents_b.append(child)\n",
    "                else:\n",
    "                    for nephew in child.children:\n",
    "                        if nephew.dep_[-2:] == 'bj':\n",
    "                            verb_dependents_b.append(nephew)\n",
    "                            \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(parse_a):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, elephant, fridge]\n",
      "[I, turkey, fridge]\n"
     ]
    }
   ],
   "source": [
    "print(verb_dependents_a)\n",
    "print(verb_dependents_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This will be method compute_similarity\"\"\"\n",
    "#similarityy of two tokens ex:\n",
    "# apples_oranges = apples.similarity(oranges)\n",
    "#dependents = verb_dependents_a\n",
    "def compute_similarity(dependents):\n",
    "    similarity = 0\n",
    "    cur, prev = None, None\n",
    "    for w in dependents:\n",
    "        #cur = parser(w) #only for testing\n",
    "        cur = w\n",
    "        #print(cur)\n",
    "        if prev == None:\n",
    "            prev = cur\n",
    "            continue\n",
    "        #print(prev)\n",
    "        similarity += prev.similarity(cur)\n",
    "        #print(similarity)\n",
    "        prev = cur\n",
    "    return similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12719840550872313 0.2729562684489863\n"
     ]
    }
   ],
   "source": [
    "dependents = ['giraffe', 'person']\n",
    "dep_b = ['niece','person']\n",
    "\n",
    "a = compute_similarity(dependents)\n",
    "b = compute_similarity(dep_b)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49096861109137535"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_a = similarity\n",
    "similarity_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.619660884141922"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_b = similarity\n",
    "similarity_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_dependent_index(parser, sent, word):\n",
    "def get_dependent_index(sent, word):\n",
    "    \n",
    "    #parse = parser(sent)\n",
    "    #print(type(sent))\n",
    "    #print('inside get_dependent_idx:', word)\n",
    "    idx = 0\n",
    "    for i, token in enumerate(sent):\n",
    "        #print( token.text)\n",
    "        if word == token.text:\n",
    "            return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url_data_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv\"\n",
    "data_task_A = pd.read_csv(url_data_task_A,header=0, index_col=0)\n",
    "#data_task_A = np.asarray(data_task_A)\n",
    "different_words = pd.read_csv('different_words.csv', index_col=0)\n",
    "#different_words = np.asarray(different_words)\n",
    "different_words_idx = np.load('idx_different_words.npy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orange juice</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a mile</td>\n",
       "      <td>100 , 000 miles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##s me</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                 1\n",
       "0   orange juice              milk\n",
       "1          apple              milk\n",
       "2         a mile   100 , 000 miles\n",
       "3         ##s me                 i"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_words[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He poured orange juice on his cereal. He poured milk on his cereal.\n",
      "[He, juice, cereal] [He, milk, cereal]\n",
      "0.6528310328722 0.6681381613016129\n",
      "He drinks apple. He drinks milk.\n",
      "[He, apple] [He, milk]\n",
      "0.1234443187713623 0.1834234744310379\n",
      "Jeff ran a mile today Jeff ran 100,000 miles today\n",
      "[Jeff, mile] [Jeff]\n",
      "0.18877871334552765 0\n",
      "A mosquito stings me I sting a mosquito\n",
      "[] []\n",
      "0.18877871334552765 0\n",
      "A niece is a person. A giraffe is a person.\n",
      "[] []\n",
      "0.18877871334552765 0\n",
      "A walk-in closet is larger than a normal closet. A normal closet is larger than a walk-in closet.\n",
      "[] []\n",
      "0.18877871334552765 0\n",
      "I like to ride my chocolate I like to ride my bike\n",
      "[chocolate] [bike]\n",
      "0 0\n",
      "A GIRL WON THE RACE WITH HER FRIEND A GIRL WON THE RACE WITH HORSE\n",
      "[] []\n",
      "0 0\n",
      "he put elephant into the jug he pour water in to the jug\n",
      "[he, elephant, jug] [he, water, jug]\n",
      "0.44731343537569046 0.27087930403649807\n",
      "A girl plays volleyball A dog plays volleyball\n",
      "[girl, volleyball] [dog, volleyball]\n",
      "0.25031012296676636 0.39596953988075256\n",
      "Eggs eat kis on Easter. Kids find eggs on Easter.\n",
      "[Eggs, kis, Easter] [Kids, eggs, Easter]\n",
      "0.08138547837734222 0.6069076955318451\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "for tuple_1, tuple_2 in zip(data_task_A.iterrows(),different_words.iterrows()):\n",
    "    sen_0 = tuple_1[1]['sent0']\n",
    "    sen_1 = tuple_1[1]['sent1']\n",
    "    \n",
    "    dif_0 = tuple_2[1][0]\n",
    "    dif_1 = tuple_2[1][1]\n",
    "    \n",
    "    #print(dif_w_idx_0, dif_w_idx_1)\n",
    "    dif_w_idx_0 = different_words_idx[tuple_1[0]][0]\n",
    "    dif_w_idx_1 = different_words_idx[tuple_1[0]][1]\n",
    "    #print(sen_0,dif_0,dif_w_idx_0,dif_w_idx_1)\n",
    "    \n",
    "    dependents_0 = get_dependents(parser, dif_0, dif_w_idx_0, sen_0)\n",
    "    dependents_1 = get_dependents(parser, dif_1, dif_w_idx_1, sen_1)\n",
    "    \n",
    "    if not type(dependents_0) == int:\n",
    "        #print(type(dependents_0))\n",
    "        if len(dependents_0) > 0:\n",
    "            sim_0 = compute_similarity(dependents_0)\n",
    "            sim_1 = compute_similarity(dependents_1)\n",
    "    \n",
    "    print(sen_0,sen_1)\n",
    "    print(dependents_0, dependents_1)\n",
    "    print(sim_0, sim_1)\n",
    "    if tuple_1[0] == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependents_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
