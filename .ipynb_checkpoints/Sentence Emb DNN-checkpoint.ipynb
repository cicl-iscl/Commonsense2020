{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is I, again....\n",
    "\n",
    "# Attention RNN with Sentence Embeddings\n",
    "\n",
    "Trying our already computed trio word embeddings with a easy rnn with attention layer...\n",
    "\n",
    "Steps:\n",
    "- Loading the data.\n",
    "    - Load answers to the tasks.\n",
    "    - Load sentence embeddings.\n",
    "    - Make sure they are the right order (how??? need to check old code, TODO)\n",
    "- Preprocessing\n",
    "    - Building correct arrays.\n",
    "    - Train/Test splits.\n",
    "- Building the Network\n",
    "    - First a really simple one to get the parameters right.\n",
    "    - Then we build a more complex one just for fun.\n",
    "- Train the Network.\n",
    "- Test the Network.\n",
    "- Cry\n",
    "    - because of overwhelming success\n",
    "    - because of overwhelming failure (more likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mf imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "url_answers_task_A = \"https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv\"\n",
    "\n",
    "answers_task_A = pd.read_csv(url_answers_task_A, index_col=0, header=None)\n",
    "answers_task_A = pd.DataFrame.to_numpy(answers_task_A)\n",
    "print(answers_task_A.shape)\n",
    "# index of the INVALID answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10000, 50, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"lets just assume they are in the right order, i mean...why shouldn't they :D ? /s....\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_0 = np.load('first column trio embdes.npy')\n",
    "sent_1 = np.load('second column trio embdes.npy')\n",
    "print(sent_0.shape)\n",
    "\"\"\"lets just assume they are in the right order, i mean...why shouldn't they :D ? /s....\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_0 = sent_0[0]\n",
    "sent_1 = sent_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Ok, now it is possible to build a network that takes two sentences as input and does something with them, but I dont want to read into that right now, so we'll use the standard way of classifying and give each sentence a True or False value, corresponding if it's valid or invalid. Easy peasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [[1,2,3],[4,5,6]]\n",
    "np.reshape(array, ( 6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_0 = sent_0[0]\n",
    "ex_1 = sent_1[0]\n",
    "ex_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((sent_0, sent_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True This returns false because it is very, very precise and we are using floats. \n",
      "Can use np.allclose though: True if thats not it maybe: False\n",
      "data shape: (20000, 50, 768) labels shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "ex_t = data[10000] # this is start of column sent_1\n",
    "ex_z = data[10001]\n",
    "print(np.array_equal(ex_1[0], ex_t[0]), 'earlier this returned false because it is very, very precise and we are using' \\\n",
    "      ' floats. \\nCan use np.allclose though:', np.allclose(ex_1[0], ex_t[0]),\n",
    "      'if thats not it maybe:', np.allclose(ex_1[0], ex_z[0]))\n",
    "\n",
    "\n",
    "labels = np.zeros((20000,))\n",
    "for i, idx_invalid in enumerate(answers_task_A):\n",
    "    if idx_invalid == 0:\n",
    "        labels[i] = 0\n",
    "        labels[i+10000] = 1\n",
    "    else:\n",
    "        labels[i] = 1\n",
    "        labels[i+10000] = 0\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('data shape:',data.shape, 'labels shape:',labels.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm pretty sure though that data should have a shape of (20000, n_feats), therefore:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (13400, 50, 768) X_test shape: (6600, 50, 768) y_train shape: (13400,) y_test shape: (6600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# takes a while, jesus these arrays are big\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape, 'y_train shape:', y_train.shape\n",
    "     , 'y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network\n",
    "\n",
    "### First a simple dnn\n",
    "\n",
    "I want to utilize a structure ... wait, a rnn with sentence embeddings doesnt make sense, lets see what cnn does for lolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Activation, Conv2D, \\\n",
    "                        Dropout, Flatten, MaxPooling2D, Bidirectional, LSTM\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "#from keras import backend as K\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(LSTM(64, return_sequences=True, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13400 samples, validate on 6600 samples\n",
      "Epoch 1/10\n",
      "13400/13400 [==============================] - 82s 6ms/sample - loss: 5.9427e-08 - accuracy: 0.5014 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 2/10\n",
      "13400/13400 [==============================] - 69s 5ms/sample - loss: 5.9427e-08 - accuracy: 0.5003 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 3/10\n",
      "13400/13400 [==============================] - 63s 5ms/sample - loss: 5.9427e-08 - accuracy: 0.5023 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 4/10\n",
      "13400/13400 [==============================] - 60s 4ms/sample - loss: 5.9427e-08 - accuracy: 0.4956 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 5/10\n",
      "13400/13400 [==============================] - 61s 5ms/sample - loss: 5.9427e-08 - accuracy: 0.5028 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 6/10\n",
      "13400/13400 [==============================] - 60s 4ms/sample - loss: 5.9427e-08 - accuracy: 0.4954 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 7/10\n",
      "13400/13400 [==============================] - 61s 5ms/sample - loss: 5.9427e-08 - accuracy: 0.4949 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 8/10\n",
      "13400/13400 [==============================] - 60s 5ms/sample - loss: 5.9427e-08 - accuracy: 0.5028 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 9/10\n",
      "13400/13400 [==============================] - 62s 5ms/sample - loss: 5.9427e-08 - accuracy: 0.4980 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Epoch 10/10\n",
      "13400/13400 [==============================] - 60s 4ms/sample - loss: 5.9427e-08 - accuracy: 0.4961 - val_loss: 5.9966e-08 - val_accuracy: 0.5042\n",
      "Test loss: 5.996588504675663e-08\n",
      "Test accuracy: 0.5042424\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                multiple                  213248    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                multiple                  33024     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                multiple                  33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  130       \n",
      "=================================================================\n",
      "Total params: 287,746\n",
      "Trainable params: 287,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.19.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.16.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow) (3.7.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (41.6.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.15.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.6)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.25.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.3.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try siamese network with functional api\n",
    "from keras.models import Model\n",
    "#import tensorflow_probability\n",
    "#tensorflow_probability=='0.8.0rc0'\n",
    "\"\"\"\n",
    "inp_0 = Input(shape=sent_0[0].shape)\n",
    "x_0 = Bidirectional(LSTM(64, return_sequences=True,dropout=0.1, recurrent_dropout=0.1))(inp_0)\n",
    "x_0 = Bidirectional(LSTM(64, return_sequences=True,dropout=0.1, recurrent_dropout=0.1))(x_0)\n",
    "x_0 = Dense(64, activation='relu')(x_0)\n",
    "x_0 = Dropout(0.1)(x_0)\n",
    "\n",
    "inp_1 = Input(shape=sent_1[0].shape)\n",
    "x_1 = Bidirectional(LSTM(64, return_sequences=True,dropout=0.1, recurrent_dropout=0.1))(inp_1)\n",
    "x_1 = Bidirectional(LSTM(64, return_sequences=True,dropout=0.1, recurrent_dropout=0.1))(x_1)\n",
    "x_1 = Dense(64, activation='relu')(x_1)\n",
    "x_1 = Dropout(0.1)(x_1)\n",
    "\"\"\"\n",
    "inp_0 = Input(shape=sent_0[0].shape)\n",
    "inp_1 = Input(shape=sent_1[0].shape)\n",
    "\n",
    "# Instantiate the layers\n",
    "lstm_1 = LSTM(64, return_sequences=True,dropout=0.1, recurrent_dropout=0.1)\n",
    "lstm_2 = LSTM(64, return_sequences=True,dropout=0.1, recurrent_dropout=0.1)\n",
    "fc_1 = Dense(64, activation='relu')\n",
    "dropout_1 = Dropout(0.1)\n",
    "\n",
    "x_0 = lstm_1(inp_0)\n",
    "x_0 = lstm_2(x_0)\n",
    "#x_0 = fc_1(x_0)\n",
    "#x_0 = dropout_1(x_0)\n",
    "\n",
    "x_1 = lstm_1(inp_1)\n",
    "x_1 = lstm_2(x_1)\n",
    "#x_1 = fc_1(x_1)\n",
    "#x_1 = dropout_1(x_1)\n",
    "\n",
    "\n",
    "#x = keras.layers.concatenate([x_0, x_1]) # for first try with two sets of seperately defined layers\n",
    "x = keras.layers.concatenate([x_0, x_1], axis=-1)\n",
    "\"\"\"\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\"\"\"\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.compat.v1.keras.Model(inputs=[inp_0,inp_1], outputs=out)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (10000, 1) was passed for an output of shape (None, 50, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-116aa6d0d69d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswers_task_A\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (10000, 1) was passed for an output of shape (None, 50, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "model.fit([sent_0,sent_1],answers_task_A,batch_size=64,epochs=3,verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
